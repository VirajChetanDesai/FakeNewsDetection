{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VirajChetanDesai/FakeNewsDetection/blob/main/Fake_News_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiVSuTuOeqV9"
      },
      "source": [
        "# Background code for N.A.T.E"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewFL_vL20vMS",
        "outputId": "d08c0104-0700-4295-f163-3eb0606fc15d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train examples: 2002\n",
            "Number of val examples: 309\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "import pickle\n",
        "\n",
        "import requests, io, zipfile\n",
        "# Download class resources...\n",
        "r = requests.get(\"https://www.dropbox.com/s/2pj07qip0ei09xt/inspirit_fake_news_resources.zip?dl=1\")\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()\n",
        "\n",
        "basepath = '.'\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "with open(os.path.join(basepath, 'train_val_data.pkl'), 'rb') as f:\n",
        "  train_data, val_data = pickle.load(f)\n",
        "\n",
        "print('Number of train examples:', len(train_data))\n",
        "print('Number of val examples:', len(val_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49IV8jyXSbt5"
      },
      "outputs": [],
      "source": [
        "def get_description_from_html(html):\n",
        "  soup = bs(html)\n",
        "  description_tag = soup.find('meta', attrs={'name':'og:description'}) or soup.find('meta', attrs={'property':'description'}) or soup.find('meta', attrs={'name':'description'})\n",
        "  if description_tag:\n",
        "    description = description_tag.get('content') or ''\n",
        "  else:\n",
        "    description = ''\n",
        "  return description\n",
        "\n",
        "def scrape_description(url):\n",
        "  if not url.startswith('http'):\n",
        "    url = 'http://' + url\n",
        "  response = requests.get(url, timeout=10)\n",
        "  html = response.text\n",
        "  description = get_description_from_html(html)\n",
        "  return description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJQY3DU1Iza9",
        "outputId": "b329d721-489e-41c6-9120-018436f86f65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 157/2002 [00:12<02:51, 10.76it/s]<ipython-input-2-11afafec2bf8>:2: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "  soup = bs(html)\n",
            "100%|██████████| 2002/2002 [02:07<00:00, 15.68it/s]\n"
          ]
        }
      ],
      "source": [
        "def get_descriptions_from_data(data):\n",
        "  descriptions = []\n",
        "  d=''\n",
        "  for site in tqdm(data):\n",
        "    descriptions.append(get_description_from_html(site[1]))\n",
        "  return descriptions\n",
        "\n",
        "\n",
        "train_descriptions = get_descriptions_from_data(train_data)\n",
        "train_urls = [url for (url, html, label) in train_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPuUgA39bO3f",
        "outputId": "f609fe20-4ab1-4fa1-8998-eaa71c2ab315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 309/309 [00:19<00:00, 15.48it/s]\n"
          ]
        }
      ],
      "source": [
        "val_descriptions = get_descriptions_from_data(val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qduz6WP1JBCO",
        "outputId": "d7a74a51-2548-4098-ffc6-e1971f44f960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preparing train data...\n",
            "\n",
            "Preparing val data...\n"
          ]
        }
      ],
      "source": [
        "vectorizer = CountVectorizer(max_features=300)\n",
        "\n",
        "vectorizer.fit(train_descriptions)\n",
        "\n",
        "def vectorize_data_descriptions(descriptions, vectorizer):\n",
        "  X = vectorizer.transform(descriptions).todense()\n",
        "  return X\n",
        "\n",
        "print('\\nPreparing train data...')\n",
        "bow_train_X = vectorize_data_descriptions(train_descriptions, vectorizer)\n",
        "bow_train_y = [label for url, html, label in train_data]\n",
        "\n",
        "print('\\nPreparing val data...')\n",
        "bow_val_X=vectorize_data_descriptions(val_descriptions, vectorizer)\n",
        "bow_val_y=[label for url, html, label in val_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWg6ZLbAJNFB",
        "outputId": "3f3d6fae-f09f-4dc3-8520-98e67446f7c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 0.8746253746253746\n",
            "Val accuracy: 0.6634304207119741\n",
            "\n",
            "Precision: 0.5844748858447488\n",
            "Recall: 0.9078014184397163\n",
            "F-Score: 0.7111111111111111\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Directly convert to numpy arrays in the fit method call\n",
        "model.fit(np.asarray(bow_train_X), bow_train_y)\n",
        "train_y_pred = model.predict(np.asarray(bow_train_X))\n",
        "print('Train accuracy:', accuracy_score(bow_train_y, train_y_pred))\n",
        "\n",
        "# Directly convert to numpy arrays in the predict method call for validation set\n",
        "val_y_pred = model.predict(np.asarray(bow_val_X))\n",
        "print('Val accuracy:', accuracy_score(bow_val_y, val_y_pred))\n",
        "\n",
        "print('')\n",
        "\n",
        "# Compute precision, recall, and F-score for the positive class\n",
        "prf = precision_recall_fscore_support(bow_val_y, val_y_pred)\n",
        "print('Precision:', prf[0][1])  # Precision for the positive class\n",
        "print('Recall:', prf[1][1])     # Recall for the positive class\n",
        "print('F-Score:', prf[2][1])    # F-Score for the positive class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIww33eRkucs",
        "outputId": "8254286c-363a-44c7-f50e-e2bbacc04d2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:38, 5.42MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:52<00:00, 7601.21it/s]\n"
          ]
        }
      ],
      "source": [
        "VEC_SIZE = 300\n",
        "glove = GloVe(name='6B', dim=VEC_SIZE)\n",
        "\n",
        "def get_word_vector(word):\n",
        "    try:\n",
        "      return glove.vectors[glove.stoi[word.lower()]].numpy()\n",
        "    except KeyError:\n",
        "      return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbM1bDocJwTy"
      },
      "outputs": [],
      "source": [
        "def glove_transform_data_descriptions(descriptions):\n",
        "    X = np.zeros((len(descriptions), VEC_SIZE))\n",
        "    for i, description in enumerate(descriptions):\n",
        "        found_words = 0.0\n",
        "        description = description.strip()\n",
        "        for word in description.split():\n",
        "            vec = get_word_vector(word)\n",
        "            if vec is not None:\n",
        "                # Increment found_words and add vec to X[i].\n",
        "                found_words += 1\n",
        "                X[i] += vec\n",
        "        # divide the sum by the number of words added, so we have the\n",
        "        # average word vector.\n",
        "        if found_words > 0:\n",
        "            X[i] /= found_words\n",
        "\n",
        "    return X\n",
        "\n",
        "glove_train_X = glove_transform_data_descriptions(train_descriptions)\n",
        "glove_train_y = [label for (url, html, label) in train_data]\n",
        "\n",
        "glove_val_X = glove_transform_data_descriptions(val_descriptions)\n",
        "glove_val_y = [label for (url, html, label) in val_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVifTxrMJ8Zb",
        "outputId": "5b751c43-47ac-4597-cb36-e40cdb9d0fa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy 0.8656343656343657\n",
            "Val accuracy 0.7702265372168284\n",
            "\n",
            "Precision: 0.7011494252873564\n",
            "Recall: 0.8652482269503546\n",
            "F-Score: 0.7746031746031746\n"
          ]
        }
      ],
      "source": [
        "model = LogisticRegression()\n",
        "\n",
        "model.fit(glove_train_X, glove_train_y)\n",
        "\n",
        "train_y_pred = model.predict(glove_train_X)\n",
        "print('Train accuracy', accuracy_score(glove_train_y, train_y_pred))\n",
        "val_y_pred = model.predict(glove_val_X)\n",
        "print('Val accuracy', accuracy_score(glove_val_y, val_y_pred))\n",
        "\n",
        "print('')\n",
        "\n",
        "prf = precision_recall_fscore_support(glove_val_y, val_y_pred)\n",
        "print('Precision:', prf[0][1])\n",
        "print('Recall:', prf[1][1])\n",
        "print('F-Score:', prf[2][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIDKiMPEKIO8"
      },
      "outputs": [],
      "source": [
        "def train_model(train_X, train_y, val_X, val_y):\n",
        "  model = LogisticRegression(solver='liblinear')\n",
        "  model.fit(train_X, train_y)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def train_and_evaluate_model(train_X, train_y, val_X, val_y):\n",
        "  model = train_model(train_X, train_y, val_X, val_y)\n",
        "\n",
        "\n",
        "  train_y_pred = model.predict(train_X)\n",
        "  print('Train accuracy', accuracy_score(train_y, train_y_pred))\n",
        "  val_y_pred = model.predict(val_X)\n",
        "  print('Val accuracy', accuracy_score(val_y, val_y_pred))\n",
        "\n",
        "  print('')\n",
        "\n",
        "  prf = precision_recall_fscore_support(val_y, val_y_pred)\n",
        "  print('Precision:', prf[0][1])\n",
        "  print('Recall:', prf[1][1])\n",
        "  print('F-Score:', prf[2][1])\n",
        "\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "3E-lU22hKcHG",
        "outputId": "e19167c7-b25c-46c9-829d-f93dc60b49ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy 0.9365634365634365\n",
            "Val accuracy 0.8705501618122977\n",
            "\n",
            "Precision: 0.8258064516129032\n",
            "Recall: 0.9078014184397163\n",
            "F-Score: 0.8648648648648649\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ncombined_train_X = combine_features([keyword_train_X, bow_train_X])\\ncombined_val_X = combine_features([keyword_val_X, bow_val_X])\\ntrain_and_evaluate_model(combined_train_X, train_y, combined_val_X, val_y)\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "def prepare_data(data, featurizer):\n",
        "    X = []\n",
        "    y = []\n",
        "    for datapoint in data:\n",
        "        url, html, label = datapoint\n",
        "        # convert all text in HTML to lowercase, so <p>Hello.</p> is mapped to\n",
        "        # <p>hello</p>. This will help us later when we extract features from\n",
        "        # the HTML, as we will be able to rely on the HTML being lowercase.\n",
        "        html = html.lower()\n",
        "        y.append(label)\n",
        "\n",
        "        features = featurizer(url, html)\n",
        "\n",
        "        # Gets the keys of the dictionary as descriptions, gets the values\n",
        "        # as the numerical features. Don't worry about exactly what zip does!\n",
        "        feature_descriptions, feature_values = zip(*features.items())\n",
        "\n",
        "        X.append(feature_values)\n",
        "\n",
        "    return X, y, feature_descriptions\n",
        "\n",
        "# Gets the log count of a phrase/keyword in HTML (transforming the phrase/keyword\n",
        "# to lowercase).\n",
        "def get_normalized_count(html, phrase):\n",
        "    return math.log(1 + html.count(phrase.lower()))\n",
        "\n",
        "# Returns a dictionary mapping from plaintext feature descriptions to numerical\n",
        "# features for a (url, html) pair.\n",
        "def keyword_featurizer(url, html):\n",
        "    features = {}\n",
        "\n",
        "    # Same as before.\n",
        "    features['.com domain'] = url.endswith('.com')\n",
        "    features['.org domain'] = url.endswith('.org')\n",
        "    features['.net domain'] = url.endswith('.net')\n",
        "    features['.info domain'] = url.endswith('.info')\n",
        "    features['.org domain'] = url.endswith('.org')\n",
        "    features['.biz domain'] = url.endswith('.biz')\n",
        "    features['.ru domain'] = url.endswith('.ru')\n",
        "    features['.co.uk domain'] = url.endswith('.co.uk')\n",
        "    features['.co domain'] = url.endswith('.co')\n",
        "    features['.tv domain'] = url.endswith('.tv')\n",
        "    features['.ac domain'] = url.endswith('.ac')\n",
        "    features['.ml domain'] = url.endswith('.ml')\n",
        "    features['.edu domain'] = url.endswith('edu.')\n",
        "\n",
        "\n",
        "\n",
        "    keywords = ['federal','<ins','potato','trump','ww1','iframe', '<video','prayer', '<source','googlesyndication','client','<audio' ,'biden', 'clinton','sports', 'finance','awesome','high','corruption','fake news','opinion','memes','instagram','riots','save','shortcut','Rahul Gandhi','modi','lower','gotta','gimme','fact','god','holy','game','clinton','jesus','podesta','infowar','bummer','<i>','AdsbyGoogle','Advertisement','senate','whatsapp','feminism','pope','facebook','legalization','wall','weed','dogs','dog','nuclear','war','president','stupid','facebook','sold','drugs','disease','dumb','retard','asshole','comments','comment','help','Tracker','superior','link','fb','finest','nazi','jew','obama','christians','muslim','muslims','claims']\n",
        "\n",
        "    for keyword in keywords:\n",
        "      features[keyword + ' keyword'] = get_normalized_count(html, keyword)\n",
        "\n",
        "\n",
        "    return features\n",
        "\n",
        "keyword_train_X, train_y, _ = prepare_data(train_data, keyword_featurizer)\n",
        "keyword_val_X, val_y, _ = prepare_data(val_data, keyword_featurizer)\n",
        "train_and_evaluate_model(keyword_train_X, train_y, keyword_val_X, val_y)\n",
        "\n",
        "\n",
        "'''\n",
        "combined_train_X = combine_features([keyword_train_X, bow_train_X])\n",
        "combined_val_X = combine_features([keyword_val_X, bow_val_X])\n",
        "train_and_evaluate_model(combined_train_X, train_y, combined_val_X, val_y)\n",
        "\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Assuming train_descriptions and val_descriptions are defined elsewhere\n",
        "vectorizer = CountVectorizer(max_features=300)\n",
        "vectorizer.fit(train_descriptions)\n",
        "\n",
        "def vectorize_data_descriptions(data_descriptions, vectorizer):\n",
        "    X = vectorizer.transform(data_descriptions).todense()\n",
        "    return np.asarray(X)  # Convert matrix to ndarray\n",
        "\n",
        "# Vectorize the data\n",
        "bow_train_X = vectorize_data_descriptions(train_descriptions, vectorizer)\n",
        "bow_val_X = vectorize_data_descriptions(val_descriptions, vectorizer)\n",
        "\n",
        "# Assuming train_and_evaluate_model and train_y, val_y are defined elsewhere\n",
        "train_and_evaluate_model(bow_train_X, train_y, bow_val_X, val_y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "cEg5jq3MhYbq",
        "outputId": "1043b15c-535c-4493-e115-5b28fa92177e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy 0.8746253746253746\n",
            "Val accuracy 0.6634304207119741\n",
            "\n",
            "Precision: 0.5844748858447488\n",
            "Recall: 0.9078014184397163\n",
            "F-Score: 0.7111111111111111\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(solver='liblinear')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "sSXDmQGlLKlL",
        "outputId": "f61a9af6-fbee-43c0-ee1a-b0418140c678"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy 0.8656343656343657\n",
            "Val accuracy 0.7702265372168284\n",
            "\n",
            "Precision: 0.7011494252873564\n",
            "Recall: 0.8652482269503546\n",
            "F-Score: 0.7746031746031746\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(solver='liblinear')"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "VEC_SIZE = 300\n",
        "glove = GloVe(name='6B', dim=VEC_SIZE)\n",
        "\n",
        "def get_word_vector(word):\n",
        "    try:\n",
        "      return glove.vectors[glove.stoi[word.lower()]].numpy()\n",
        "    except KeyError:\n",
        "      return None\n",
        "\n",
        "def glove_transform_data_descriptions(descriptions):\n",
        "    X = np.zeros((len(descriptions), VEC_SIZE))\n",
        "    for i, description in enumerate(descriptions):\n",
        "        found_words = 0.0\n",
        "        description = description.strip()\n",
        "        for word in description.split():\n",
        "            vec = get_word_vector(word)\n",
        "            if vec is not None:\n",
        "                # Increment found_words and add vec to X[i].\n",
        "                found_words += 1\n",
        "                X[i] += vec\n",
        "        # divide the sum by the number of words added, so we have the\n",
        "        # average word vector.\n",
        "        if found_words > 0:\n",
        "            X[i] /= found_words\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "\n",
        "glove_train_X = glove_transform_data_descriptions(train_descriptions)\n",
        "glove_val_X = glove_transform_data_descriptions(val_descriptions)\n",
        "\n",
        "train_and_evaluate_model(glove_train_X, train_y, glove_val_X, val_y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD1gl146LW2E",
        "outputId": "abc8b5fd-f84f-4693-f89e-26ec71ef2912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy 0.9560439560439561\n",
            "Val accuracy 0.889967637540453\n",
            "\n",
            "Precision: 0.8451612903225807\n",
            "Recall: 0.9290780141843972\n",
            "F-Score: 0.8851351351351351\n"
          ]
        }
      ],
      "source": [
        "def combine_features(X_list):\n",
        "  return np.concatenate(X_list, axis=1)\n",
        "\n",
        "combined_train_X = combine_features([keyword_train_X, bow_train_X, glove_train_X])\n",
        "combined_val_X = combine_features([keyword_val_X, bow_val_X, glove_val_X])\n",
        "\n",
        "model = train_and_evaluate_model(combined_train_X, train_y, combined_val_X, val_y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "cWFviCZ8DauJ",
        "outputId": "807179a2-dd2f-4754-b32e-37358607dd89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preparing train data...\n",
            "\n",
            "Preparing val data...\n",
            "Train accuracy 0.8761238761238761\n",
            "Val accuracy 0.7087378640776699\n",
            "\n",
            "Precision: 0.6231884057971014\n",
            "Recall: 0.9148936170212766\n",
            "F-Score: 0.7413793103448276\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(solver='liblinear')"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(train_descriptions)\n",
        "\n",
        "def vectorize_data_descriptions(descriptions, vectorizer):\n",
        "    X = vectorizer.transform(descriptions).todense()\n",
        "    return np.asarray(X)\n",
        "\n",
        "print('\\nPreparing train data...')\n",
        "bow_train_X = vectorize_data_descriptions(train_descriptions, vectorizer)\n",
        "bow_train_y = [label for url, html, label in train_data]\n",
        "\n",
        "print('\\nPreparing val data...')\n",
        "bow_val_X = vectorize_data_descriptions(val_descriptions, vectorizer)\n",
        "bow_val_y = [label for url, html, label in val_data]\n",
        "\n",
        "train_and_evaluate_model(bow_train_X, bow_train_y, bow_val_X, bow_val_y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "UOmNnnGvLpZd",
        "outputId": "a247e10e-ff24-4e74-d4e4-3940d5433a85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy 0.957042957042957\n",
            "Val accuracy 0.9061488673139159\n",
            "\n",
            "Precision: 0.8733333333333333\n",
            "Recall: 0.9290780141843972\n",
            "F-Score: 0.9003436426116839\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(solver='liblinear')"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "def prepare_data(data, featurizer):\n",
        "    X = []\n",
        "    y = []\n",
        "    for datapoint in data:\n",
        "        url, html, label = datapoint\n",
        "        # We convert all text in HTML to lowercase, so <p>Hello.</p> is mapped to\n",
        "        # <p>hello</p>. This will help us later when we extract features from\n",
        "        # the HTML, as we will be able to rely on the HTML being lowercase.\n",
        "        html = html.lower()\n",
        "        y.append(label)\n",
        "\n",
        "        features = featurizer(url, html)\n",
        "\n",
        "        # Gets the keys of the dictionary as descriptions, gets the values\n",
        "        # as the numerical features. Don't worry about exactly what zip does!\n",
        "        feature_descriptions, feature_values = zip(*features.items())\n",
        "\n",
        "        X.append(feature_values)\n",
        "\n",
        "    return X, y, feature_descriptions\n",
        "\n",
        "# Gets the log count of a phrase/keyword in HTML (transforming the phrase/keyword\n",
        "# to lowercase).\n",
        "def get_normalized_count(html, phrase):\n",
        "    return math.log(1 + html.count(phrase.lower()))\n",
        "\n",
        "# Returns a dictionary mapping from plaintext feature descriptions to numerical\n",
        "# features for a (url, html) pair.\n",
        "def keyword_featurizer(url, html):\n",
        "    features = {}\n",
        "\n",
        "    # Same as before.\n",
        "    features['.com domain'] = url.endswith('.com')\n",
        "    features['.org domain'] = url.endswith('.org')\n",
        "    features['.net domain'] = url.endswith('.net')\n",
        "    features['.info domain'] = url.endswith('.info')\n",
        "    features['.org domain'] = url.endswith('.org')\n",
        "    features['.biz domain'] = url.endswith('.biz')\n",
        "    features['.ru domain'] = url.endswith('.ru')\n",
        "    features['.co.uk domain'] = url.endswith('.co.uk')\n",
        "    features['.co domain'] = url.endswith('.co')\n",
        "    features['.tv domain'] = url.endswith('.tv')\n",
        "    features['.news domain'] = url.endswith('.news')\n",
        "    features['.ac domain'] = url.endswith('.ac')\n",
        "    features['.ml domain'] = url.endswith('.ml')\n",
        "    features['.edu domain'] = url.endswith('edu.')\n",
        "\n",
        "\n",
        "    keywords = ['federal','wtf','marijuania','please','<ins','potato','trump','ww1','iframe', '<video','prayer', '<source','googlesyndication','client','<audio' ,'biden', 'clinton','sports', 'finance','awesome','high','corruption','fake news','opinion','memes','instagram','riots','save','shortcut','Rahul Gandhi','modi','lower','gotta','gimme','fact','god','holy','game','clinton','jesus','podesta','infowar','bummer','<i>','AdsbyGoogle','Advertisement','senate','whatsapp','feminism','pope','facebook','legalization','wall','weed','dogs','dog','nuclear','war','president','stupid','facebook','sold','drugs','disease','dumb','retard','asshole','comments','comment','help','Tracker','superior','link','fb','finest','nazi','jew','obama','christians','muslim','muslims','claims']\n",
        "\n",
        "    for keyword in keywords:\n",
        "      features[keyword + ' keyword'] = get_normalized_count(html, keyword)\n",
        "\n",
        "\n",
        "    return features\n",
        "\n",
        "keyword_train_X, train_y, _ = prepare_data(train_data, keyword_featurizer)\n",
        "keyword_val_X, val_y, _ = prepare_data(val_data, keyword_featurizer)\n",
        "'''\n",
        "train_and_evaluate_model(keyword_train_X, train_y, keyword_val_X, val_y)\n",
        "'''\n",
        "\n",
        "\n",
        "combined_train_X = combine_features([keyword_train_X, bow_train_X])\n",
        "combined_val_X = combine_features([keyword_val_X, bow_val_X])\n",
        "train_and_evaluate_model(combined_train_X, train_y, combined_val_X, val_y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu8h9wNZcm7y",
        "outputId": "d54444a0-69f1-4110-fb8c-c234950e051e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "google.com appears to be real.\n"
          ]
        }
      ],
      "source": [
        "#@title Live Fake News Classification Demo { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "\n",
        "def get_data_pair(url):\n",
        "  if not url.startswith('http'):\n",
        "      url = 'http://' + url\n",
        "  url_pretty = url\n",
        "  if url_pretty.startswith('http://'):\n",
        "      url_pretty = url_pretty[7:]\n",
        "  if url_pretty.startswith('https://'):\n",
        "      url_pretty = url_pretty[8:]\n",
        "\n",
        "  # Scrape website for HTML\n",
        "  response = requests.get(url, timeout=10)\n",
        "  htmltext = response.text\n",
        "\n",
        "  return url_pretty, htmltext\n",
        "\n",
        "curr_url = \"google.com\" #@param {type:\"string\"}\n",
        "\n",
        "url, html = get_data_pair(curr_url)\n",
        "\n",
        "# Call on the output of *keyword_featurizer* or something similar\n",
        "# to transform it into a format that allows for concatenation. See\n",
        "# example below.\n",
        "def dict_to_features(features_dict):\n",
        "  X = np.array(list(features_dict.values())).astype('float')\n",
        "  X = X[np.newaxis, :]\n",
        "  return X\n",
        "def featurize_data_pair(url, html):\n",
        "  # Approach 1.\n",
        "  keyword_X = dict_to_features(keyword_featurizer(url, html))\n",
        "\n",
        "\n",
        "  # Approach 2.\n",
        "  description = get_description_from_html(html)\n",
        "\n",
        "  bow_X = vectorize_data_descriptions([description], vectorizer)\n",
        "\n",
        "  # Approach 3.\n",
        "  '''\n",
        "  glove_X = glove_transform_data_descriptions([description])\n",
        "  '''\n",
        "  X = combine_features([keyword_X, bow_X])\n",
        "\n",
        "  return X\n",
        "\n",
        "curr_X = featurize_data_pair(url, html)\n",
        "\n",
        "model = train_model(combined_train_X, train_y, combined_val_X, val_y)\n",
        "\n",
        "curr_y = model.predict(curr_X)[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if curr_y < 0.5 :\n",
        "  print(curr_url, 'appears to be real.')\n",
        "else:\n",
        "  print(curr_url, 'appears to be fake.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTqHvYzGnpYD",
        "outputId": "0fcee470-61fa-4c27-ce14-707439b71bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading test data...\n",
            "Done loading test data...\n",
            "Test accuracy 0.7886178861788617\n",
            "Confusion matrix:\n",
            "[[ 83  51]\n",
            " [  1 111]]\n",
            "Precision: 0.6851851851851852\n",
            "Recall: 0.9910714285714286\n",
            "F-Score: 0.8102189781021897\n"
          ]
        }
      ],
      "source": [
        "\n",
        "with open(os.path.join(basepath, 'test_data.pkl'), 'rb') as f:\n",
        "  test_data = pickle.load(f)\n",
        "\n",
        "model = train_model(combined_train_X, train_y, combined_val_X, val_y)\n",
        "\n",
        "print('Loading test data...')\n",
        "test_X = []\n",
        "for url, html, label in test_data:\n",
        "  curr_X = np.array(featurize_data_pair(url, html))\n",
        "  test_X.append(curr_X[0])\n",
        "\n",
        "test_X = np.array(test_X)\n",
        "\n",
        "test_y = [label for url, html, label in test_data]\n",
        "\n",
        "print('Done loading test data...')\n",
        "\n",
        "test_y_pred = model.predict(test_X)\n",
        "\n",
        "print('Test accuracy', accuracy_score(test_y, test_y_pred))\n",
        "\n",
        "print('Confusion matrix:')\n",
        "print(confusion_matrix(test_y, test_y_pred))\n",
        "\n",
        "prf = precision_recall_fscore_support(test_y, test_y_pred)\n",
        "\n",
        "print('Precision:', prf[0][1])\n",
        "print('Recall:', prf[1][1])\n",
        "print('F-Score:', prf[2][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyAZrW36e90H"
      },
      "source": [
        "# Runtime code for N.A.T.E"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example\n",
        "https://www.dazeddigital.com/life-culture/article/60896/1/lab-tests-prove-that-the-alien-corpses-are-real-if-you-want-them-to-be\n",
        "\n"
      ],
      "metadata": {
        "id": "OaD4xghmiov4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZHGg1BDfCJQ",
        "outputId": "ca310f57-ad94-46b7-cfde-fcc7e41e962a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello I am N.A.T.E\n",
            "I am here to help you discern fake news websites from real news websites...\n",
            "\n",
            "Copy and paste the website url here!\n",
            " https://www.dazeddigital.com/life-culture/article/60896/1/lab-tests-prove-that-the-alien-corpses-are-real-if-you-want-them-to-be\n",
            "\n",
            "processing...\n",
            "\n",
            "As far as I can detect the content in the url: \n",
            "https://www.dazeddigital.com/life-culture/article/60896/1/lab-tests-prove-that-the-alien-corpses-are-real-if-you-want-them-to-be appears to be fake.\n",
            "\n",
            "                           Always stay vigilant for fake news...\n",
            "                                 And for now... Goodbye!\n"
          ]
        }
      ],
      "source": [
        "#@title :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: N.A.T.E :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "\n",
        "import time\n",
        "import sys\n",
        "'''\n",
        "def my_except_hook(exctype, value, traceback):\n",
        "        print('There has been an error in the system')\n",
        "sys.excepthook = my_except_hook\n",
        "'''\n",
        "print('Hello I am N.A.T.E')\n",
        "print('I am here to help you discern fake news websites from real news websites...')\n",
        "time.sleep(2)\n",
        "print('')\n",
        "print('Copy and paste the website url here!')\n",
        "\n",
        "\n",
        "\n",
        "def get_data_pair(url):\n",
        "  if not url.startswith('http'):\n",
        "      url = 'http://' + url\n",
        "  url_pretty = url\n",
        "  if url_pretty.startswith('http://'):\n",
        "      url_pretty = url_pretty[7:]\n",
        "  if url_pretty.startswith('https://'):\n",
        "      url_pretty = url_pretty[8:]\n",
        "  # Scrape website for HTML\n",
        "  response = requests.get(url, timeout=10)\n",
        "  htmltext = response.text\n",
        "\n",
        "  return url_pretty, htmltext\n",
        "\n",
        "curr_url = input(str(' '))\n",
        "print('')\n",
        "url, html = get_data_pair(curr_url)\n",
        "\n",
        "# Call on the output of *keyword_featurizer* or something similar\n",
        "# to transform it into a format that allows for concatenation. See\n",
        "# example below.\n",
        "def dict_to_features(features_dict):\n",
        "  X = np.array(list(features_dict.values())).astype('float')\n",
        "  X = X[np.newaxis, :]\n",
        "  return X\n",
        "def featurize_data_pair(url, html):\n",
        "  # Approach 1.\n",
        "  keyword_X = dict_to_features(keyword_featurizer(url, html))\n",
        "\n",
        "\n",
        "  # Approach 2.\n",
        "  description = get_description_from_html(html)\n",
        "\n",
        "  bow_X = vectorize_data_descriptions([description], vectorizer)\n",
        "\n",
        "  # Approach 3.\n",
        "  '''\n",
        "  glove_X = glove_transform_data_descriptions([description])\n",
        "  '''\n",
        "  X = combine_features([keyword_X, bow_X])\n",
        "\n",
        "  return X\n",
        "\n",
        "curr_X = featurize_data_pair(url, html)\n",
        "\n",
        "model = train_model(combined_train_X, train_y, combined_val_X, val_y)\n",
        "\n",
        "curr_y = model.predict(curr_X)[0]\n",
        "\n",
        "print('processing...')\n",
        "time.sleep(0.5)\n",
        "print('')\n",
        "print('As far as I can detect the content in the url: ')\n",
        "\n",
        "if curr_y < 0.5 :\n",
        "  print(curr_url, 'appears to be real.')\n",
        "else:\n",
        "  print(curr_url, 'appears to be fake.')\n",
        "\n",
        "time.sleep(3)\n",
        "print('')\n",
        "print('                           Always stay vigilant for fake news...')\n",
        "time.sleep(1)\n",
        "print('                                 And for now... Goodbye!')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "WiVSuTuOeqV9"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}